from typing import List, Dict, Any, Optional
import requests
import aiohttp
import ssl
import certifi
from .base_parser import BaseParser
from bs4 import BeautifulSoup
from config import HEADERS
from utils.keywords import KEYWORDS, EXCLUDE_WORDS
import logging

# –°–æ–∑–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç SSL, –∫–æ—Ç–æ—Ä—ã–π –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞
ssl_context = ssl.create_default_context()
ssl_context.check_hostname = False
ssl_context.verify_mode = ssl.CERT_NONE

logger = logging.getLogger(__name__)

class FreelanceRuParser(BaseParser):
    def __init__(self):
        super().__init__('Freelance.ru', 'https://freelance.ru')
        self.search_url = f'{self.base_url}/projects/'

    def find_projects(self):
        """–ò—â–µ—Ç –∑–∞–∫–∞–∑—ã –Ω–∞ freelance.ru"""
        try:
            self.logger.info("üîç –ò—â—É –∑–∞–∫–∞–∑—ã –Ω–∞ freelance.ru...")
            response = requests.get(self.search_url, headers=HEADERS)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            projects = []
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø—Ä–æ–µ–∫—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            project_items = soup.select('.content .proj')
            
            for item in project_items:
                try:
                    title = item.select_one('.ptitle a')
                    desc = item.select_one('.ptxt')
                    
                    if title and desc:
                        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                        full_text = (title.text + ' ' + desc.text).lower()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏—Å–∫–ª—é—á–∞—é—â–∏—Ö —Å–ª–æ–≤
                        if (any(keyword in full_text for keyword in KEYWORDS) and
                            not any(exclude in full_text for exclude in EXCLUDE_WORDS)):
                            
                            link = f'{self.base_url}{title["href"]}'
                            projects.append({
                                'title': title.text.strip(),
                                'link': link,
                                'description': desc.text.strip()
                            })
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–µ–∫—Ç–∞: {e}")
                    continue
            
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(projects)} –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–∞ freelance.ru")
            return projects
            
        except requests.RequestException as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å freelance.ru: {e}")
            return []

    async def async_find_projects(self) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        try:
            conn = aiohttp.TCPConnector(ssl=ssl_context)
            async with aiohttp.ClientSession(connector=conn) as session:
                async with session.get(
                    self.search_url,
                    headers=HEADERS,
                    ssl=ssl_context if __name__ != '__main__' else None
                ) as response:
                    if response.status == 200:
                        projects = await self._async_parse_response(await response.text())
                        filtered = self._filter_projects(projects, KEYWORDS, EXCLUDE_WORDS)
                        self._log_projects(filtered)
                        return filtered
                    else:
                        self.logger.error(f"HTTP –æ—à–∏–±–∫–∞: {response.status}")
                        return []
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞: {e}")
            raise

    async def _async_parse_response(self, response_text):
        soup = BeautifulSoup(response_text, 'html.parser')
        projects = []
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø—Ä–æ–µ–∫—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        project_items = soup.select('.content .proj')
        
        for item in project_items:
            try:
                title = item.select_one('.ptitle a')
                desc = item.select_one('.ptxt')
                
                if title and desc:
                    link = f'{self.base_url}{title["href"]}'
                    projects.append({
                        'title': title.text.strip(),
                        'link': link,
                        'description': desc.text.strip(),
                    })
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–µ–∫—Ç–∞: {e}")
                continue

        return projects

    async_parse = async_find_projects  # –ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å main.py
