from typing import List, Dict, Optional
import urllib.parse
from bs4 import BeautifulSoup
from utils.keywords import KEYWORDS, EXCLUDE_WORDS
from .base_parser import BaseParser


class FLRuParser(BaseParser):
    def __init__(self):
        super().__init__("FL.ru", "https://www.fl.ru")
        self.search_url = f"{self.base_url}/projects/"

    async def _parse_project_card(self, card) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –∫–∞—Ä—Ç–æ—á–∫—É –ø—Ä–æ–µ–∫—Ç–∞"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫—É
            title_elem = card.select_one('a.b-post__link')
            if not title_elem:
                return None
                
            title = title_elem.get_text(strip=True)
            relative_url = title_elem.get('href', '')
            
            # –ü–∞—Ä—Å–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ
            desc_elem = card.select_one('div.b-post__txt')
            description = desc_elem.get_text(strip=True) if desc_elem else ''
            
            # –ü–∞—Ä—Å–∏–º —Ü–µ–Ω—É, –µ—Å–ª–∏ –µ—Å—Ç—å
            price_elem = card.select_one('div.b-post__price')
            price = price_elem.get_text(strip=True) if price_elem else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            full_text = f"{title} {description}".lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if (any(keyword in full_text for keyword in KEYWORDS) and 
                not any(exclude in full_text for exclude in EXCLUDE_WORDS)):
                
                return {
                    'title': title,
                    'link': urllib.parse.urljoin(self.base_url, relative_url),
                    'description': description,
                    'price': price,
                    'source': 'FL.ru'
                }
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞: {e}")
        
        return None

    async def async_find_projects(self) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏—â–µ—Ç –ø—Ä–æ–µ–∫—Ç—ã –Ω–∞ FL.ru"""
        self.logger.info("üîç –ò—â—É –∑–∞–∫–∞–∑—ã –Ω–∞ FL.ru...")
        projects = []
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ—Ç–ª–∞–¥–∫–∏
            html = await self._make_request(self.search_url)
            if not html:
                return []
                
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤
            project_cards = soup.select('div.project-card')
            if not project_cards:
                # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                project_cards = soup.select('div.b-post')
                
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤: {len(project_cards)}")
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –∫–∞—Ä—Ç–æ—á–∫—É
            for card in project_cards:
                project = await self._parse_project_card(card)
                if project:
                    projects.append(project)
                    
            self._log_projects(projects)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ FL.ru: {e}", exc_info=True)
            
        return projects
        
    def find_projects(self) -> List[Dict]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        import asyncio
        return asyncio.run(self.async_find_projects())

    async def async_find_projects(self) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    self.search_url,
                    headers=HEADERS,
                    ssl=ssl_context,
                ) as response:
                    if response.status == 200:
                        projects = await self._async_parse_response(await response.text())
                        filtered = self._filter_projects(projects, KEYWORDS, EXCLUDE_WORDS)
                        self._log_projects(filtered)
                        return filtered
                    else:
                        self.logger.error(f"HTTP –æ—à–∏–±–∫–∞: {response.status}")
                        return []
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞: {e}")
            raise

    async def _async_parse_response(self, html):
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        soup = BeautifulSoup(html, 'html.parser')
        projects = []
        
        for project in soup.select('.b-post'):
            try:
                title = project.select_one('.b-post__link')
                desc = project.select_one('.b-post__txt')
                price = project.select_one('.b-post__price')
                
                if title and desc:
                    project = {
                        'title': title.text.strip(),
                        'link': f"{self.base_url}{title['href']}",
                        'description': desc.text.strip(),
                        'price': price.text.strip() if price else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                    }
                    projects.append(project)
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞: {e}")
        
        return projects
