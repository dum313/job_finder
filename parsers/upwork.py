from typing import List, Dict, Optional, Any, Union
import random
import time
import json
import re
import asyncio
import aiohttp
import ssl
from urllib.parse import urlencode, urljoin, parse_qs, urlparse
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from aiohttp_socks import ProxyConnector
from utils.keywords import KEYWORDS, EXCLUDE_WORDS
from .base_parser import BaseParser

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º UserAgent
ua = UserAgent()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã
REQUEST_DELAY = 5  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
MAX_RETRIES = 3     # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø–æ–≤—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞

# –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã)
PROXIES = [
    # 'socks5://user:pass@host:port',
    # 'http://user:pass@host:port',
]

def get_random_headers() -> Dict[str, str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'TE': 'trailers',
        'DNT': '1',
        'Sec-GPC': '1',
    }


class UpworkParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π Upwork —Å –æ–±—Ö–æ–¥–æ–º –∑–∞—â–∏—Ç—ã."""

    def __init__(self, proxy: Optional[str] = None) -> None:
        super().__init__("Upwork", "https://www.upwork.com")
        self.search_url = f"{self.base_url}/nxsites/messages/threads/api/threads"
        self.proxy = proxy or (PROXIES[0] if PROXIES else None)
        self.connector = None
        self.cookies = {}
        self.session = None
        self.initialized = False

    async def _init_session(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Å—Å–∏—é —Å –Ω—É–∂–Ω—ã–º–∏ –∫—É–∫–∞–º–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
        if self.initialized and self.session and not self.session.closed:
            return True
            
        try:
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
            if self.proxy:
                if self.proxy.startswith('socks'):
                    self.connector = ProxyConnector.from_url(self.proxy, ssl=False)
                else:
                    self.connector = aiohttp.TCPConnector(ssl=False)
                
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
            self.session = aiohttp.ClientSession(
                connector=self.connector,
                headers=get_random_headers(),
                cookie_jar=aiohttp.CookieJar(unsafe=True),
                trust_env=True
            )
            
            # –î–µ–ª–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫—É–∫–æ–≤
            async with self.session.get(
                self.base_url,
                allow_redirects=True,
                ssl=ssl.SSLContext()
            ) as response:
                if response.status != 200:
                    self.logger.error(f"Failed to initialize session: {response.status}")
                    return False
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—É–∫–∏
                self.cookies = {c.key: c.value for c in self.session.cookie_jar}
                
                # –ü–æ–ª—É—á–∞–µ–º CSRF —Ç–æ–∫–µ–Ω –∏–∑ HTML
                html = await response.text()
                self._save_debug_html(html, "init")
                
                # –ü–∞—Ä—Å–∏–º CSRF —Ç–æ–∫–µ–Ω
                csrf_match = re.search(r'"csrfToken":"([^"]+)"', html)
                if csrf_match:
                    self.cookies['XSRF-TOKEN'] = csrf_match.group(1)
                    self.session.headers.update({
                        'X-Requested-With': 'XMLHttpRequest',
                        'X-Odesk-Csrf-Token': self.cookies['XSRF-TOKEN']
                    })
                
                self.initialized = True
                return True
                
        except Exception as e:
            self.logger.error(f"Error initializing session: {e}", exc_info=True)
            if self.session:
                await self.session.close()
            self.initialized = False
            return False

    def _parse_project(self, item: Dict[str, Any]) -> Optional[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏–∑ JSON-–æ—Ç–≤–µ—Ç–∞"""
        try:
            title = item.get('title', '')
            description = item.get('description', '')
            url = item.get('ciphertext')
            
            if not all([title, description, url]):
                return None
                
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
            full_url = urljoin(self.base_url, f'/job/{url}')
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            full_text = f"{title} {description}".lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if (any(keyword in full_text for keyword in KEYWORDS) and 
                not any(exclude in full_text for exclude in EXCLUDE_WORDS)):
                
                return {
                    'title': title,
                    'link': full_url,
                    'description': description,
                    'price': item.get('amount', {}).get('amount', '–î–æ–≥–æ–≤–æ—Ä–Ω–∞—è'),
                    'source': 'Upwork'
                }
                
        except Exception as e:
            self.logger.error(f"Error parsing project: {e}")
            
        return None

    async def _make_upwork_request(self, url: str, params: Optional[Dict] = None) -> Optional[Dict]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ API Upwork —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        if not await self._init_session():
            return None
            
        for attempt in range(MAX_RETRIES):
            try:
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if attempt > 0:
                    await asyncio.sleep(REQUEST_DELAY * (attempt + 1))
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                headers = get_random_headers()
                if 'X-Odesk-Csrf-Token' in self.session.headers:
                    headers['X-Odesk-Csrf-Token'] = self.session.headers['X-Odesk-Csrf-Token']
                
                async with self.session.get(
                    url,
                    params=params,
                    headers=headers,
                    ssl=ssl.SSLContext()
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data
                    elif response.status in [403, 429]:
                        self.logger.warning(f"Rate limited or blocked. Waiting before retry... (Attempt {attempt + 1}/{MAX_RETRIES})")
                        await asyncio.sleep(10 * (attempt + 1))  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ
                    else:
                        self.logger.error(f"Request failed with status {response.status}")
                        break
                        
            except Exception as e:
                self.logger.error(f"Request error: {e}")
                if attempt == MAX_RETRIES - 1:
                    return None
                
        return None

    async def async_find_projects(self) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–∞ Upwork"""
        self.logger.info("üîç –ò—â—É –∑–∞–∫–∞–∑—ã –Ω–∞ Upwork...")
        projects = []
        
        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            params = {
                'page': '1',
                't': str(int(time.time() * 1000)),
                'filter': json.dumps({
                    'job_type': ['hourly', 'fixed'],
                    'job_status': ['open'],
                    'sort': 'create_time desc',
                    'q': ' OR '.join(KEYWORDS),
                    'limit': 50,
                    'offset': 0
                })
            }
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            data = await self._make_upwork_request(self.search_url, params)
            if not data or 'threads' not in data:
                self.logger.error("Invalid response format or no projects found")
                return []
            
            # –ü–∞—Ä—Å–∏–º –ø—Ä–æ–µ–∫—Ç—ã
            for item in data['threads']:
                project = self._parse_project(item)
                if project:
                    projects.append(project)
            
            self._log_projects(projects)
            
        except Exception as e:
            self.logger.error(f"Error in Upwork parser: {e}", exc_info=True)
            
        return projects
        
    def find_projects(self) -> List[Dict]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        import asyncio
        return asyncio.run(self.async_find_projects())
        try:
            self.logger.info("üîç –ò—â—É –∑–∞–∫–∞–∑—ã –Ω–∞ Upwork...")
            proxies = {'http': self.proxy, 'https': self.proxy} if self.proxy else None
            
            # –ü—Ä–æ–±—É–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ User-Agents
            for attempt in range(3):
                try:
                    headers = get_random_headers()
                    self.logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} —Å User-Agent: {headers['User-Agent'][:50]}...")
                    
                    response = requests.get(
                        self.search_url,
                        headers=headers,
                        proxies=proxies,
                        timeout=30
                    )
                    response.raise_for_status()
                    
                    if 'captcha' in response.text.lower() or 'access denied' in response.text.lower():
                        self.logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞ –∏–ª–∏ –¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω. –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π User-Agent...")
                        continue
                        
                    projects = self._parse_html(response.text)
                    filtered = self._filter_projects(projects, KEYWORDS, EXCLUDE_WORDS)
                    self._log_projects(filtered)
                    return filtered
                    
                except requests.RequestException as e:
                    self.logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                    if attempt == 2:  # –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        raise
                    
            return []
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ Upwork: {e}", exc_info=True)
            return []

    async def async_find_projects(self) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–∞ Upwork."""
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –¥–ª—è aiohttp
        conn = None
        if self.proxy:
            from aiohttp_socks import ProxyConnector
            connector = ProxyConnector.from_url(self.proxy, ssl=ssl_context)
        else:
            connector = aiohttp.TCPConnector(ssl=ssl_context)

        try:
            for attempt in range(3):
                try:
                    headers = get_random_headers()
                    self.logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} —Å User-Agent: {headers['User-Agent'][:50]}...")
                    
                    async with aiohttp.ClientSession(connector=connector) as session:
                        async with session.get(
                            self.search_url,
                            headers=headers,
                            timeout=aiohttp.ClientTimeout(total=30)
                        ) as response:
                            text = await response.text()
                            
                            if response.status != 200:
                                self.logger.warning(f"–û—à–∏–±–∫–∞ {response.status} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Upwork")
                                continue
                                
                            if 'captcha' in text.lower() or 'access denied' in text.lower():
                                self.logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞ –∏–ª–∏ –¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω. –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π User-Agent...")
                                continue
                                
                            projects = self._parse_html(text)
                            filtered = self._filter_projects(projects, KEYWORDS, EXCLUDE_WORDS)
                            self._log_projects(filtered)
                            return filtered
                            
                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    self.logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                    if attempt == 2:  # –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        raise
                    await asyncio.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                    
            return []
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –ø–∞—Ä—Å–∏–Ω–≥–µ Upwork: {e}", exc_info=True)
            return []
            
        finally:
            if conn:
                await conn.close()

    def _parse_html(self, html: str) -> List[Dict]:
        soup = BeautifulSoup(html, "html.parser")
        projects: List[Dict] = []
        for item in soup.select("section.up-card-section"):
            try:
                title_tag = item.select_one("a.job-title-link") or item.select_one("a")
                desc_tag = item.select_one("p")
                price_tag = item.select_one(".amount")
                if title_tag and desc_tag:
                    href = title_tag["href"]
                    projects.append({
                        "title": title_tag.text.strip(),
                        "link": self.base_url + href,
                        "description": desc_tag.text.strip(),
                        "price": price_tag.text.strip() if price_tag else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞",
                    })
            except Exception as e:  # pragma: no cover - logging only
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞: {e}")
        return projects
